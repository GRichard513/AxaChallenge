{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet AXA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from scipy import io\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from datetime import datetime\n",
    "import error_functions as ef\n",
    "from dateutil import relativedelta\n",
    "from workalendar.europe import France\n",
    "# pip install workalendar\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# colors ofr plot\n",
    "blue_light = '#029aed'\n",
    "orange_med = '#ff5722'\n",
    "green_light = '#63a600'\n",
    "gray_light = '#666666'\n",
    "cal = France()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create DataFrame with public holliday in France for years 2011, 2012, 2013, 2014\n",
    "holiday_map = pd.DataFrame()\n",
    "for year in [2011,2012,2013,2014]:\n",
    "    holiday_map_temp = pd.DataFrame(cal.holidays(year))\n",
    "    holiday_map_temp = holiday_map_temp.set_index([0])\n",
    "    holiday_map = pd.concat([holiday_map, holiday_map_temp], ignore_index=False)\n",
    "# holiday_map.head()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# loading the train data\n",
    "data = pd.read_csv('data/train_add_duplicates.csv', sep=\";\", parse_dates=['DATE'], index_col = ['DATE'], encoding='latin-1')\n",
    "data['DATE'] = data.index\n",
    "# data.head()\n",
    "\n",
    "# loading the test data\n",
    "submission = pd.read_csv('data/submission.txt', sep=\"\\t\", parse_dates=['DATE'], index_col = ['DATE'])\n",
    "submission['DATE'] = submission.index\n",
    "# submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# timestamp exctraction\n",
    "def splitDatetime(data) :\n",
    "    datatime = pd.DatetimeIndex(data.DATE)\n",
    "    data['year'] = datatime.year\n",
    "    data['month'] = datatime.month\n",
    "    data['day'] = datatime.day\n",
    "    data['hour'] = datatime.hour\n",
    "    data['min'] = datatime.minute\n",
    "    data['dayweek'] = datatime.weekday\n",
    "    data['workingday'] = (datatime.weekday < 6).astype(int)\n",
    "    data['holiday'] = data.index.isin(holiday_map.index).astype(int)\n",
    "    data['night'] = (datatime.hour < 7).astype(int)\n",
    "    return data\n",
    "\n",
    "data = splitDatetime(data)\n",
    "submission = splitDatetime(submission)\n",
    "# submission.head()\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def format_label():\n",
    "    # pivot table on data for ASS_ASSIGNMENT columnwise\n",
    "    X = pd.DataFrame()\n",
    "    X = data[['year','month','day','hour','ASS_ASSIGNMENT','CSPL_CALLS','min','DATE']]\n",
    "    df1 = X.pivot_table(index = ['DATE'], columns = ['ASS_ASSIGNMENT'], values = ['CSPL_CALLS'], aggfunc=np.sum)\n",
    "    # print(df1.shape)\n",
    "    # when data for this date an categorie not available, fill with 0\n",
    "    df1.fillna(0, inplace=True)\n",
    "    # df1.head()\n",
    "\n",
    "    # creating labels, ASS_ASSIGNMENT are presented columnwise thanks to pivot table above\n",
    "    y_df = pd.DataFrame()\n",
    "    for cat in data.ASS_ASSIGNMENT.unique() :\n",
    "        y_df[cat] = df1['CSPL_CALLS'][cat]\n",
    "    # y_df.head()\n",
    "    \n",
    "    return y_df\n",
    "    \n",
    "y_df = format_label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition Trend/Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    for cat in y_df.keys():\n",
    "        print(cat)\n",
    "        plt.figure(figsize=(20,10))\n",
    "        plt.plot(y_df[cat][:48*21])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    # trying to infer frequence (not working)\n",
    "    inferred_freq = y_df.index.inferred_freq\n",
    "    used_freq = 336\n",
    "    print('inferred freq : %s' %inferred_freq)\n",
    "    print('used freq : %s' %used_freq)\n",
    "\n",
    "\n",
    "    # seasonal decomposition\n",
    "    y_df.Nuit.interpolate(inplace=True)\n",
    "    decomposition = sm.tsa.seasonal_decompose(y_df['2011-01':'2011-04'].CAT, freq=used_freq)\n",
    "\n",
    "    trend = decomposition.trend\n",
    "    seasonal = decomposition.seasonal \n",
    "    residual = decomposition.resid \n",
    "\n",
    "    # plot trend, season, resid\n",
    "    fig = decomposition.plot()\n",
    "    fig.set_size_inches(12, 7)\n",
    "    fig.tight_layout()\n",
    "    plt.style.use('ggplot')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data first date : 2012-12-28 00:00:00\n",
      "test data last date  : 2013-12-28 23:30:00\n",
      "test data range : 1 years, 0 months and 0 days\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def format_subission():\n",
    "    \n",
    "    # formating submission data\n",
    "    X_test = pd.DataFrame(index = submission.index)\n",
    "    X_test['DATE'] = submission.index\n",
    "    X_test = splitDatetime(X_test)\n",
    "    X_test.drop('DATE', axis=1, inplace=True)\n",
    "    X_test = X_test.drop_duplicates()\n",
    "\n",
    "    date_min_test = X_test.index.min()\n",
    "    date_max_test = X_test.index.max()\n",
    "    X_test_range = relativedelta.relativedelta(date_max_test, date_min_test)\n",
    "\n",
    "    # print submission data total range\n",
    "    print('test data first date : %s' %date_min_test)\n",
    "    print('test data last date  : %s' %date_max_test)\n",
    "    print('test data range : %s years, %s months and %s days\\n' %(X_test_range.years, X_test_range.months,X_test_range.days))\n",
    "    # X_test.head()\n",
    "    \n",
    "    return X_test\n",
    "\n",
    "X_test = format_subission()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data first date : 2011-01-01 00:00:00\n",
      "train data last date  : 2013-12-31 23:30:00\n",
      "train data range : 2 years, 11 months and 30 days\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def format_train():\n",
    "    \n",
    "    # formatting train data\n",
    "    X_train = pd.DataFrame(index = y_df.index)\n",
    "    X_train['DATE'] = y_df.index\n",
    "    X_train = splitDatetime(X_train)\n",
    "    X_train.drop('DATE', axis=1, inplace=True)\n",
    "    \n",
    "    date_min = X_train.index.min()\n",
    "    date_max = X_train.index.max()\n",
    "    X_train_range = relativedelta.relativedelta(date_max, date_min)\n",
    "\n",
    "    # print train data range\n",
    "    print('train data first date : %s' %date_min)\n",
    "    print('train data last date  : %s' %date_max)\n",
    "    print('train data range : %s years, %s months and %s days\\n' %(X_train_range.years, X_train_range.months,X_train_range.days))\n",
    "    # X_train.head()\n",
    "    \n",
    "    return X_train\n",
    "    \n",
    "X_train = format_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# function to split data for CV purposes\n",
    "# algorithm is test on last week of the randomly choosen train data set\n",
    "\n",
    "def split_data(X_train):\n",
    "\n",
    "    # the subset contain at leat 50% of the original data\n",
    "    s = np.random.uniform(X_train.shape[0]/2,X_train.shape[0],1).astype(int)\n",
    "    X_train.drop(X_train.index[s:], inplace=True)\n",
    "\n",
    "    date_min = X_train.index.min()\n",
    "    date_max = X_train.index.max()\n",
    "    X_train_range = relativedelta.relativedelta(date_max, date_min)\n",
    "\n",
    "    # print subset range\n",
    "    print('train data first date : %s' %date_min)\n",
    "    print('train data last date  : %s' %date_max)\n",
    "    print('train data range : %s years, %s months and %s days\\n' %(X_train_range.years, X_train_range.months,X_train_range.days))\n",
    "    # X_train.head()\n",
    "    \n",
    "    # Last week of the subset is used for cross validation purposes\n",
    "    X_train_CV = X_train.last('7d')\n",
    "    date_min_CV = X_train_CV.index.min()\n",
    "    date_max_CV = X_train_CV.index.max()\n",
    "    X_CV_range = relativedelta.relativedelta(date_max_CV, date_min_CV)\n",
    "\n",
    "    # print CV subset week range\n",
    "    print('CV data first date : %s' %date_min_CV)\n",
    "    print('CV data last date  : %s' %date_max_CV)\n",
    "    print('CV data range : %s years, %s months and %s days' %(X_CV_range.years, X_CV_range.months,X_CV_range.days))\n",
    "    # X_train_CV.head()\n",
    "    \n",
    "    return X_train, X_train_CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing data anterior to prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_ant(X_train, X_test):\n",
    "\n",
    "    # removing data from train set anterior to data of the submission file\n",
    "    date_min_test = X_test.index.min()\n",
    "    X_train = X_train.truncate(after=date_min_test)\n",
    "    y_df = y_df.truncate(after=date_min_test)\n",
    "\n",
    "    date_min = X_train.index.min()\n",
    "    date_max = X_train.index.max()\n",
    "    X_train_range = relativedelta.relativedelta(date_max, date_min)\n",
    "\n",
    "    # printing range of data after removing non-causal data\n",
    "    print('train data first date : %s' %date_min)\n",
    "    print('train data last date  : %s' %date_max)\n",
    "    print('train data range : %s years, %s months and %s days\\n' %(X_train_range.years, X_train_range.months,X_train_range.days))\n",
    "    \n",
    "    return X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding features from last week\n",
    "\n",
    "/!\\ function can take a few min to execute !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_features():\n",
    "    \n",
    "    # Set number of hours prediction is in advance\n",
    "    n_periods_advance = 1\n",
    "\n",
    "    # Set number of historic hours used\n",
    "    n_periods_window = 4\n",
    "\n",
    "    for cat in y_df:\n",
    "        if cat not in ['Evenements','Gestion Amex']:\n",
    "            for k in range(n_periods_advance,n_periods_advance+n_periods_window):\n",
    "                X_test['%s_t-%i'%(cat,k)] = y_df[cat].shift(k*30, freq='min')\n",
    "                X_train['%s_t-%i'%(cat,k)] = y_df[cat].shift(k)\n",
    "\n",
    "    # remove raw following week without data\n",
    "    y_df.drop(X_train[pd.isnull(X_train).any(axis=1)].index, inplace=True)\n",
    "    X_train.drop(X_train[pd.isnull(X_train).any(axis=1)].index, inplace=True)\n",
    "\n",
    "    return X_train, y_df\n",
    "\n",
    "#X_train, y_df = add_features()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_last_week(X_train, y_train, nbweeks=1):\n",
    "    \n",
    "    for cat in y_train.keys():\n",
    "        if cat not in ['Evenements','Gestion Amex']:\n",
    "            for k in range(nbweeks):\n",
    "                X_train['%s_t-%i'%(cat,7*k)] = y_train[cat].shift(7*k, 'd')\n",
    "        #print(cat+ ' - OK')\n",
    "    \n",
    "    # remove raw following week without data\n",
    "    y_df2=y_df.drop(X_train[pd.isnull(X_train).any(axis=1)].index)\n",
    "    X_train2=X_train.drop(X_train[pd.isnull(X_train).any(axis=1)].index)\n",
    "    return X_train2, y_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing as pre\n",
    "\n",
    "class FeatureExtractor(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X_df, y_df):\n",
    "        self.y=y_df\n",
    "        pass\n",
    "    \n",
    "    def transform(self, X_df):\n",
    "        X_df,_=add_last_week(X_df,self.y,nbweeks=3)\n",
    "        if 'day' in X_df.keys():\n",
    "            for k in range(1,32):\n",
    "                if str(k) not in X_df.keys():\n",
    "                    X_df[str(k)]=np.zeros(len(X_df))\n",
    "                    \n",
    "            dd=pd.get_dummies(X_df['day'])\n",
    "            for a in dd.keys():\n",
    "                X_df[str(a)]=dd[a]\n",
    "            X_df.drop('day',axis=1,inplace=True)\n",
    "        \n",
    "        if 'dayweek' in X_df.keys():\n",
    "            dd=pd.get_dummies(X_df['dayweek'])\n",
    "            for a in dd.keys():\n",
    "                X_df['d'+str(a)]=dd[a]\n",
    "            X_df.drop('dayweek',axis=1,inplace=True)\n",
    "        \n",
    "        if 'month' in X_df.keys():\n",
    "            dd=pd.get_dummies(X_df['month'])\n",
    "            for a in dd.keys():\n",
    "                X_train['m'+str(a)]=dd[a]\n",
    "            X_df.drop('month',axis=1,inplace=True)\n",
    "\n",
    "        \n",
    "        return X_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "#from xgboost import plot_importance\n",
    "\n",
    "class Regressor(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.n_components = 20\n",
    "        pca = PCA(n_components=self.n_components)\n",
    "        selection = SelectKBest(k = 5)\n",
    "        combined_features = FeatureUnion([(\"pca\", pca), (\"univ_select\", selection)])\n",
    "        \n",
    "        self.reg = Pipeline([\n",
    "            #('cf', combined_features),\n",
    "            #('pca', PCA(n_components = self.n_components)),\n",
    "            ('xgb', xgb.XGBRegressor(\n",
    "                learning_rate = 0.1,\n",
    "                n_estimators = 100,\n",
    "                max_depth = 2,\n",
    "                min_child_weight = 1,\n",
    "                gamma = 0.2,\n",
    "                subsample = 0.9,\n",
    "            ))\n",
    "        ])\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.reg.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.reg.predict(X)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self.reg.predict_proba(X)\n",
    "    \n",
    "    def grid_search_fit(self, X, y):\n",
    "        y = y.astype(float)\n",
    "        # use a full grid over all parameters\n",
    "        param_grid = dict(\n",
    "            # pca__n_components = [10,20,40],\n",
    "            xgb__max_depth = [3,10,2],#[10,11,9]#[9,10,11],#\n",
    "            xgb__min_child_weight = [1,6,2],\n",
    "            # xgb__gamma = [i/10.0 for i in range(0,5)]\n",
    "            # xgb__subsample = [i/10.0 for i in range(6,10)],\n",
    "            # xgb__colsample_bytree = [i/10.0 for i in range(6,10)]\n",
    "            )\n",
    "\n",
    "        param_grid_rfr = dict(\n",
    "            max_leaf_nodes = [9,10,11],\n",
    "            max_depth = [3, None],\n",
    "            max_features = [1, 3, 10],\n",
    "            min_samples_split = [3, 10],\n",
    "            min_samples_leaf = [1, 3, 10],\n",
    "            bootstrap = [True, False]\n",
    "            )\n",
    "\n",
    "        # error definition\n",
    "        linex = make_scorer(ef.linex_loss, greater_is_better = False)\n",
    "\n",
    "        grid_search = GridSearchCV(self.reg, param_grid = param_grid, scoring = linex)#, verbose = 10)\n",
    "\n",
    "        # fit gridsearch\n",
    "        self.reg = grid_search.fit(X, y)\n",
    "\n",
    "        print('Grid search  best score: %.3f' % grid_search.best_score_)\n",
    "        print('Grid search  best params:')\n",
    "        for k, v in sorted(grid_search.best_params_.items()):\n",
    "            print(\"\\t%s: %r\" % (k, v))\n",
    "        # get best estimator\n",
    "        self.reg = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guillaume/anaconda3/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "class Regressor(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.n_components = 5\n",
    "        \n",
    "        self.reg_day = Pipeline([\n",
    "            ('xgb', xgb.XGBRegressor(\n",
    "                learning_rate = 0.1,\n",
    "                n_estimators = 100,\n",
    "                max_depth = 2,\n",
    "                min_child_weight = 1,\n",
    "                gamma = 0.2,\n",
    "                subsample = 0.9,\n",
    "            ))\n",
    "        ])\n",
    "        \n",
    "        self.reg_night = Pipeline([\n",
    "            ('xgb', xgb.XGBRegressor(\n",
    "                learning_rate = 0.1,\n",
    "                n_estimators = 100,\n",
    "                max_depth = 2,\n",
    "                min_child_weight = 1,\n",
    "                gamma = 0.2,\n",
    "                subsample = 0.9,\n",
    "            ))\n",
    "        ])\n",
    "        \n",
    "        # self.reg_day = xgb.XGBRegressor(learning_rate = 0.1, n_estimators = 100, max_depth = 2, min_child_weight = 1, gamma = 0.2, subsample = 0.9)\n",
    "        # self.reg_night = xgb.XGBRegressor(learning_rate = 0.1, n_estimators = 100, max_depth = 2, min_child_weight = 1, gamma = 0.2, subsample = 0.9)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        is_day = np.array(X.night == 0)\n",
    "        self.reg_day.fit(X[is_day], y[is_day])\n",
    "        self.reg_night.fit(X[~is_day], y[~is_day])\n",
    "\n",
    "    def predict(self, X):\n",
    "        is_day = np.array(X.night == 0)\n",
    "        res = np.zeros(X.shape[0])\n",
    "        \n",
    "        # to write submission file, X size = 1\n",
    "        # Specific case to avoid array of bool beeing interpreted as bool\n",
    "        if (is_day.size == 1):\n",
    "            if is_day:\n",
    "                res = self.reg_day.predict(X)\n",
    "            if ~is_day:\n",
    "                res = self.reg_night.predict(X)\n",
    "        else:     \n",
    "            day_pred = self.reg_day.predict(X[is_day])\n",
    "            night_pred = self.reg_night.predict(X[~is_day])\n",
    "            res[is_day] = day_pred\n",
    "            res[~is_day] = night_pred\n",
    "            \n",
    "        return res\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self.reg.predict_proba(X)\n",
    "    \n",
    "    def grid_search_fit(self, X, y):\n",
    "        param_grid = dict(\n",
    "            xgb__n_estimators = [100, 200, 400],\n",
    "            xgb__learning_rate = [i/10.0 for i in range(0,5)]\n",
    "            # pca__n_components = [10,20,40],\n",
    "            # xgb__max_depth = [3,10,2],#[10,11,9]#[9,10,11],#\n",
    "            # xgb__min_child_weight = [1,6,2],\n",
    "            # xgb__gamma = [i/10.0 for i in range(0,5)]\n",
    "            # xgb__subsample = [i/10.0 for i in range(6,10)],\n",
    "            # xgb__colsample_bytree = [i/10.0 for i in range(6,10)]\n",
    "            )\n",
    "\n",
    "        # error definition\n",
    "        linex = make_scorer(ef.linex_loss, greater_is_better = False)\n",
    "        \n",
    "        # perform grid search\n",
    "        grid_search_day = GridSearchCV(self.reg_day, param_grid = param_grid, scoring = linex)#, verbose = 10)\n",
    "        grid_search_night = GridSearchCV(self.reg_night, param_grid = param_grid, scoring = linex)#, verbose = 10)\n",
    "\n",
    "        # fit gridsearch\n",
    "        is_day = np.array(X.night == 0)\n",
    "        self.reg_day = grid_search_day.fit(X[is_day], y[is_day])\n",
    "        self.reg_night = grid_search_night.fit(X[~is_day], y[~is_day])\n",
    "\n",
    "        print('\\t Grid search day best score: %.3f' % grid_search_day.best_score_)\n",
    "        print('\\t Grid search day best params:')\n",
    "        for k, v in sorted(grid_search_day.best_params_.items()):\n",
    "            print(\"\\t \\t%s: %r\" % (k, v))\n",
    "        # get best estimator\n",
    "        self.reg_day = grid_search_day.best_estimator_\n",
    "        \n",
    "        print('\\t Grid search night best score: %.3f' % grid_search_night.best_score_)\n",
    "        print('\\t Grid search night best params:')\n",
    "        for k, v in sorted(grid_search_night.best_params_.items()):\n",
    "            print(\"\\t \\t%s: %r\" % (k, v))\n",
    "        # get best estimator\n",
    "        self.reg_day = grid_search_night.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guillaume/anaconda3/lib/python3.5/site-packages/pandas/tseries/base.py:212: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n",
      "  result = getitem(key)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data first date : 2011-01-01 00:00:00\n",
      "train data last date  : 2012-05-20 01:30:00\n",
      "train data range : 1 years, 4 months and 19 days\n",
      "\n",
      "CV data first date : 2012-05-13 02:00:00\n",
      "CV data last date  : 2012-05-20 01:30:00\n",
      "CV data range : 0 years, 0 months and 6 days\n",
      "Crises\n",
      "------------------------------------------------\n",
      "Done.\n",
      "Exctracting features ...\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'X_test_array_reg' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-7938a84287ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[0mskf_is\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m \u001b[0mtrain_test_model_clf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskf_is\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFeatureExtractor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRegressor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-18-7938a84287ea>\u001b[0m in \u001b[0;36mtrain_test_model_clf\u001b[1;34m(X_df, y_df, skf_is, FeatureExtractor, Regressor, GS)\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0mfe_reg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[0mX_train_array_reg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfe_reg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_array_reg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m             \u001b[0mX_test_array_reg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfe_reg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Done.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'X_test_array_reg' referenced before assignment"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "def train_test_model_clf(X_df, y_df, skf_is, FeatureExtractor, Regressor, GS):\n",
    "    y_train_reg = {}\n",
    "    y_test_reg = {}\n",
    "    reg = {}\n",
    "    y_pred_reg = {}\n",
    "    error = 0\n",
    "    count =0\n",
    "    for cat in data.ASS_ASSIGNMENT.unique():\n",
    "        print('%s' %cat)\n",
    "        if cat not in ['Evenements','Gestion Amex']:\n",
    "            print('------------------------------------------------')\n",
    "            # Spliting data for cross validation\n",
    "            train_is, test_is = skf_is\n",
    "            \n",
    "            # test/train definition\n",
    "            X_train_df = X_df.iloc[train_is].copy()\n",
    "            # for prediction only use general features such as date and time and category last week data\n",
    "            base_col = ['year','month','day','hour','min','workingday','holiday','dayweek','weekday','night']\n",
    "            filter_col = [col for col in list(X_train_df) if (col.startswith(cat) or col in base_col)]\n",
    "            X_train_df = X_train_df[filter_col]\n",
    "            y_train_df = y_df.iloc[train_is].copy()\n",
    "            X_test_df = X_df.iloc[test_is].copy()\n",
    "            X_test_df = X_test_df[filter_col]\n",
    "            y_test_df = y_df.iloc[test_is].copy()\n",
    "            \n",
    "            \n",
    "            # for téléphonie only use last 6 month because data range to vary a lot each year\n",
    "            if cat in ['Téléphonie']: #Tech. Axa','CAT'\n",
    "                X_train_df = X_train_df.last('6m')\n",
    "                y_train_df = y_train_df.last('6m')\n",
    "\n",
    "            # label category definition\n",
    "            y_train_reg[cat] = y_train_df[cat].values\n",
    "            y_test_reg[cat] = y_test_df[cat].values\n",
    "            # y_test_reg = y_test_df['count'].values\n",
    "            print(\"Done.\")\n",
    "\n",
    "            # Features extraction (no modification of data in this case)\n",
    "            print(\"Exctracting features ...\"),\n",
    "            fe_reg = FeatureExtractor()\n",
    "            fe_reg.fit(X_train_df, y_train_df)\n",
    "            X_train_array_reg = fe_reg.transform(X_train_df)\n",
    "            X_test_array_reg = fe_reg.transform(X_test_df)\n",
    "            print(\"Done.\")\n",
    "\n",
    "            # Train\n",
    "            # regressors initialisation\n",
    "            reg[cat] = Regressor()\n",
    "\n",
    "            # grid search to calibrate model before fitting (if set to True in function)\n",
    "            if GS :\n",
    "                if cat in ['Tech. Axa','Téléphonie','CAT']:\n",
    "                    reg[cat].grid_search_fit(X_train_array_reg, y_train_reg[cat])\n",
    "\n",
    "            # fitting model\n",
    "            print(\"Training algorithm for %s...\" %cat)\n",
    "            reg[cat].fit(X_train_array_reg, y_train_reg[cat])\n",
    "            print(\"Done.\")\n",
    "            # Test\n",
    "            print(\"Testing algorithm for %s...\" %cat),\n",
    "            y_pred_reg[cat] = np.round(np.maximum(reg[cat].predict(X_test_array_reg),0),0)\n",
    "            error_tmp = ef.linex_loss(y_pred_reg[cat], y_test_reg[cat])\n",
    "            error += error_tmp\n",
    "\n",
    "            print(\"Done.\")\n",
    "            print('error %s = %.1f' %(cat,error_tmp))\n",
    "            \n",
    "            if count==0:\n",
    "                y_test_plt = pd.DataFrame(index = y_test_df.index)\n",
    "                y_pred_plt = pd.DataFrame(index = y_test_df.index)\n",
    "                count=1\n",
    "            # plot figure for predicted week\n",
    "            y_test_plt[cat] = y_test_reg[cat]\n",
    "            y_pred_plt[cat] = y_pred_reg[cat]\n",
    "            \n",
    "            fig = plt.figure(figsize=[15,4])\n",
    "            plt.style.use('ggplot')\n",
    "            plt.plot(y_test_plt[cat])\n",
    "            plt.plot(y_pred_plt[cat], linestyle = 'dashed', color=gray_light, linewidth=2)\n",
    "            plt.title('error on predected week for %s (error = %.1f)' %(cat,error_tmp))\n",
    "            plt.legend(['true','pred'],loc='best')\n",
    "            plt.show()\n",
    "            \n",
    "            print('\\n------------------------------------------------')\n",
    "        else:\n",
    "            print(\"pass.\")\n",
    "            print('------------------------------------------------')\n",
    "    k = X_train_df.shape[0]+X_test_df.shape[0]\n",
    "    l = X_train_df.shape[0]\n",
    "    print('train sample size %% total sample size = %.2f%%' %(100*float(l)/k))\n",
    "    print('error = %.1f' %(error))\n",
    "    \n",
    "    \n",
    "    return \n",
    "# splitting sample for CV : \n",
    "# take a radom part of the train sample (at least 50% of the original data size) and the last week to test results\n",
    "X_train, X_train_CV = split_data(X_train)\n",
    "a = X_train.shape[0]\n",
    "b = X_train_CV.shape[0]\n",
    "skf_is = ([np.arange(a-b).astype(int),np.arange(a-b,a).astype(int)])\n",
    "\n",
    "train_test_model_clf(X_train, y_df, skf_is, FeatureExtractor, Regressor, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_features(X_train, X_test, y_train, y_pred):\n",
    "    \n",
    "    # Set number of hours prediction is in advance\n",
    "    n_periods_advance = 1\n",
    "\n",
    "    # Set number of historic semi-hours used\n",
    "    n_periods_window = 4\n",
    "\n",
    "    for cat in y_train.keys():\n",
    "        if cat not in ['Evenements','Gestion Amex']:\n",
    "            for k in range(n_periods_advance,n_periods_advance+n_periods_window):\n",
    "                X_test['%s_t-%i'%(cat,k)] = y_pred[cat].shift(k*30, freq='min')\n",
    "                X_train['%s_t-%i'%(cat,k)] = y_train[cat].shift(k)\n",
    "            for j in range(1,5):\n",
    "                for k in range(j):\n",
    "                    X_test['%s_t-%i'%(cat,j)].ix[k]=y_train[cat].ix[len(y_train)-j+k]\n",
    "    print(cat+ ' - OK')\n",
    "    # remove raw following week without data\n",
    "    y_df2=y_df.drop(X_train[pd.isnull(X_train).any(axis=1)].index)\n",
    "    X_train2=X_train.drop(X_train[pd.isnull(X_train).any(axis=1)].index)\n",
    "    return X_train2, X_test, y_df2, y_pred\n",
    "\n",
    "#X_train, y_df = add_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    error = 0\n",
    "    X_train_df2, X_test_df2, y_train_reg2, y_pred_reg2=add_features(X_train_df, X_test_df, y_df, y_pred_plt)\n",
    "    for cat in data.ASS_ASSIGNMENT.unique():\n",
    "        print('%s' %cat)\n",
    "        if cat not in ['Evenements','Gestion Amex']:\n",
    "            print('------------------------------------------------')\n",
    "            \n",
    "            X_train_array_reg2 = fe_reg.transform(X_train_df2)\n",
    "            X_test_array_reg2 = fe_reg.transform(X_test_df2)\n",
    "\n",
    "            # Train\n",
    "            # regressors initialisation\n",
    "            reg[cat] = Regressor()\n",
    "\n",
    "            # grid search to calibrate model before fitting (if set to True in function)\n",
    "            if GS :\n",
    "                if cat in ['Tech. Axa','Téléphonie','CAT']:\n",
    "                    reg[cat].grid_search_fit(X_train_array_reg2, y_train_reg2[cat])\n",
    "\n",
    "            # fitting model\n",
    "            print(\"Training algorithm for %s...\" %cat)\n",
    "            reg[cat].fit(X_train_array_reg2, y_train_reg2[cat])\n",
    "            #print(\"Done.\")\n",
    "            # Test\n",
    "            print(\"Testing algorithm for %s...\" %cat),\n",
    "            y_pred_reg[cat] = np.round(np.maximum(reg[cat].predict(X_test_array_reg2),0),0)\n",
    "            error_tmp = ef.linex_loss(y_pred_reg2[cat], y_test_reg[cat])\n",
    "            error += error_tmp\n",
    "\n",
    "            #print(\"Done.\")\n",
    "            print('error %s = %.1f' %(cat,error_tmp))\n",
    "\n",
    "            if count==0:\n",
    "                y_test_plt = pd.DataFrame(index = y_test_df.index)\n",
    "                y_pred_plt = pd.DataFrame(index = y_test_df.index)\n",
    "                count=1\n",
    "            # plot figure for predicted week\n",
    "            y_test_plt[cat] = y_test_reg[cat]\n",
    "            y_pred_plt[cat] = y_pred_reg2[cat]\n",
    "\n",
    "            fig = plt.figure(figsize=[15,4])\n",
    "            plt.style.use('ggplot')\n",
    "            plt.plot(y_test_plt[cat])\n",
    "            plt.plot(y_pred_plt[cat], linestyle = 'dashed', color=gray_light, linewidth=2)\n",
    "            plt.title('2. error on predected week for %s (error = %.1f)' %(cat,error_tmp))\n",
    "            plt.legend(['true','pred'],loc='best')\n",
    "            plt.show()\n",
    "\n",
    "            print('\\n------------------------------------------------')\n",
    "        else:\n",
    "            print(\"pass.\")\n",
    "            print('------------------------------------------------')\n",
    "    k = X_train_df.shape[0]+X_test_df.shape[0]\n",
    "    l = X_train_df.shape[0]\n",
    "    print('train sample size %% total sample size = %.2f%%' %(100*float(l)/k))\n",
    "    print('error = %.1f' %(error))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'y_df' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-1dab2debc7d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;31m# removing data from the train set anterior to data from submission set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_ant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[0mtrain_test_model_clf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFeatureExtractor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-40-c4b6bc2e7de1>\u001b[0m in \u001b[0;36mremove_ant\u001b[1;34m(X_train, X_test)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mdate_min_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mafter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdate_min_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0my_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mafter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdate_min_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mdate_min\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'y_df' referenced before assignment"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "def train_test_model_clf(X_train, X_test, y_train, FeatureExtractor):\n",
    "    \n",
    "    y_train_reg = {}\n",
    "    y_test_reg = {}\n",
    "    reg = {}\n",
    "    y_pred_reg = {}\n",
    "    error = 0\n",
    "    first = True\n",
    "    y_pred = np.zeros((submission.shape[0]))\n",
    "    \n",
    "    for cat in submission.ASS_ASSIGNMENT.unique():\n",
    "        print('%s' %cat.decode('utf-8'))\n",
    "        if cat not in ['Evenements','Gestion Amex']:\n",
    "            print('------------------------------------------------')\n",
    "\n",
    "            # test/train definition  \n",
    "            X_train_df = X_train.copy()\n",
    "            y_train_df = y_train.copy()\n",
    "            X_test_df = X_test.copy()\n",
    "            \n",
    "            base_col = ['year','month','day','hour','min','workingday','holiday','weekday','night']\n",
    "            filter_col = [col for col in list(X_train_df) if (col.startswith(cat.decode('utf-8')) or col in base_col)]          \n",
    "            X_test_df = X_test_df[filter_col]\n",
    "            X_train_df = X_train_df[filter_col]\n",
    "            \n",
    "            if cat in ['Téléphonie']: #'Tech. Axa','CAT'\n",
    "                X_train_df = X_train_df.last('6m')\n",
    "                y_train_df = y_train_df.last('6m')\n",
    "\n",
    "            # cat definition\n",
    "            y_train_reg[cat] = y_train_df[cat].values\n",
    "            print(\"Done.\")\n",
    "\n",
    "            # Features extraction (no modification of data in this case)\n",
    "            print(\"Exctracting features ...\"),\n",
    "            fe_reg = FeatureExtractor()\n",
    "            fe_reg.fit(X_train_df, y_train_df)\n",
    "            X_train_array_reg = fe_reg.transform(X_train_df)\n",
    "            X_test_array_reg = fe_reg.transform(X_test_df)\n",
    "            print(\"Done.\")\n",
    "\n",
    "            # Train\n",
    "            print(\"Training algorithm for %s...\" %cat.decode('utf-8')),\n",
    "            # regressors initialisation\n",
    "            reg[cat] = Regressor()\n",
    "\n",
    "            # uncomment to perform grid search to calibrate model before fitting\n",
    "            #if cat in ['Tech. Axa','Téléphonie','CAT']:\n",
    "            #    reg[cat].grid_search_fit(X_train_array_reg, y_train_reg[cat])\n",
    "\n",
    "            # fitting model\n",
    "            reg[cat].fit(X_train_array_reg, y_train_reg[cat])            \n",
    "            print(\"Done.\")\n",
    "            print('\\n------------------------------------------------')\n",
    "        else:\n",
    "            print(\"pass.\")\n",
    "            print('------------------------------------------------')\n",
    "    rep = pd.DataFrame(index = submission.index)\n",
    "    rep['DATE'] = submission.index\n",
    "    rep['ASS_ASSIGNMENT'] = submission.ASS_ASSIGNMENT\n",
    "    i = 0\n",
    "    \n",
    "    for index, row in rep.iterrows():\n",
    "        cat = row['ASS_ASSIGNMENT']\n",
    "        y_pred[i] = reg[cat].predict(X_test.loc[index])[0].astype(int)\n",
    "        i+=1\n",
    "    rep['prediction'] = np.maximum(y_pred.astype(int),0)\n",
    "    #rep['DATE'] = [dd + \".000\" for dd in rep['DATE']]\n",
    "    print(\"saved in file\")\n",
    "    \n",
    "    rep.to_csv(\"submission_test.txt\", sep=\"\\t\", index=False)\n",
    "\n",
    "# formating data from the submission file\n",
    "# X_test = format_subission()\n",
    "\n",
    "# removing data from the train set anterior to data from submission set\n",
    "X_train = remove_ant(y_df, X_test)\n",
    "\n",
    "train_test_model_clf(X_train, X_test, y_df, FeatureExtractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_file = \"submission_test.txt\"\n",
    "output_file = \"submission_test_modif.txt\"\n",
    "\n",
    "modif = pd.read_csv(input_file, sep=\"\\t\")\n",
    "print(\"File read.\")\n",
    "modif['DATE'] = [dd + \".000\" for dd in modif['DATE']]\n",
    "print(\"Data modified.\")\n",
    "modif.to_csv(output_file, sep=\"\\t\", index=False)\n",
    "print(\"All done.\")\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
